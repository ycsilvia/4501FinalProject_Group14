{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {
    "id": "32f8ca24"
   },
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {
    "id": "25627e8d"
   },
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {
    "id": "2f75fd94"
   },
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "GFafiViVRYcu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFafiViVRYcu",
    "outputId": "971effd7-96bf-4997-d2c1-6abb718ecf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\silvia\\anaconda3\\lib\\site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pyarrow) (1.21.5)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\silvia\\anaconda3\\lib\\site-packages (2022.11.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fastparquet) (1.21.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fastparquet) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from packaging->fastparquet) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Requirement already satisfied: geopandas in c:\\users\\silvia\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas) (1.5.2)\n",
      "Requirement already satisfied: fiona>=1.8 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas) (1.8.22)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas) (3.4.0)\n",
      "Requirement already satisfied: shapely>=1.7 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas) (1.8.5.post1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six>=1.7 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: munch in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (2022.9.14)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (8.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (63.4.1)\n",
      "Requirement already satisfied: attrs>=17 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from packaging->geopandas) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from click>=4.0->fiona>=1.8->geopandas) (0.4.5)\n",
      "Requirement already satisfied: pytest in c:\\users\\silvia\\anaconda3\\lib\\site-packages (7.1.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pytest) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from packaging->pytest) (3.0.9)\n",
      "Requirement already satisfied: keplergl in c:\\users\\silvia\\anaconda3\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: geopandas>=0.5.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from keplergl) (0.12.1)\n",
      "Requirement already satisfied: traittypes>=0.2.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from keplergl) (0.2.1)\n",
      "Requirement already satisfied: ipywidgets<8,>=7.0.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from keplergl) (7.6.5)\n",
      "Requirement already satisfied: pandas>=0.23.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from keplergl) (1.5.2)\n",
      "Requirement already satisfied: Shapely>=1.6.4.post2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from keplergl) (1.8.5.post1)\n",
      "Requirement already satisfied: fiona>=1.8 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas>=0.5.0->keplergl) (1.8.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas>=0.5.0->keplergl) (21.3)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from geopandas>=0.5.0->keplergl) (3.4.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (1.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (7.31.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (3.5.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (6.15.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipywidgets<8,>=7.0.0->keplergl) (5.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=0.23.0->keplergl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=0.23.0->keplergl) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from pandas>=0.23.0->keplergl) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (63.4.1)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (0.7.2)\n",
      "Requirement already satisfied: munch in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (2.5.0)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (8.0.4)\n",
      "Requirement already satisfied: attrs>=17 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (21.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (2022.9.14)\n",
      "Requirement already satisfied: six>=1.7 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas>=0.5.0->keplergl) (1.16.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (0.1.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (23.2.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (1.5.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (6.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (1.5.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (7.4.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (0.4.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (2.11.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (0.18.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.0.0->keplergl) (4.16.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.0.0->keplergl) (2.16.2)\n",
      "Requirement already satisfied: jupyter_core in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.0.0->keplergl) (4.11.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (6.4.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from packaging->geopandas>=0.5.0->keplergl) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets<8,>=7.0.0->keplergl) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->keplergl) (0.4)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from jupyter_core->nbformat>=4.2.0->ipywidgets<8,>=7.0.0->keplergl) (302)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.14.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (21.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (2.11.3)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (1.8.0)\n",
      "Requirement already satisfied: nbconvert>=5 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (6.4.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8,>=7.0.0->keplergl) (0.2.5)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.5.13)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.7.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (4.11.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (2.0.1)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (2.3.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\silvia\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->keplergl) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# install needed libraries\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install geopandas\n",
    "!pip install pytest\n",
    "!pip install keplergl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dcde05",
   "metadata": {
    "id": "66dcde05"
   },
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import keplergl\n",
    "from keplergl import KeplerGl\n",
    "import statistics\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {
    "id": "3f1242c4"
   },
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {
    "id": "26ad10ea"
   },
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {
    "id": "ecf38168"
   },
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what you’re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {
    "id": "32074561"
   },
   "source": [
    "### Calculating distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {
    "id": "4cbbe6cc"
   },
   "outputs": [],
   "source": [
    "# Calculate the distance between the two coordinates\n",
    "def calculate_distance(from_coord: list, to_coord: list) -> float:\n",
    "    R = 6373.0\n",
    "    lat1 = math.radians(from_coord[0])\n",
    "    lon1 = math.radians(from_coord[1])\n",
    "    lat2 = math.radians(to_coord[0])\n",
    "    lon2 = math.radians(to_coord[1])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0095630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6abf52",
   "metadata": {
    "id": "6d6abf52"
   },
   "outputs": [],
   "source": [
    "# For the dataset that is not given the trip distance, calculate the distance using the given coordinate data and add it to the dataframe\n",
    "def add_distance_column(dataframe: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    distance = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        distance.append(calculate_distance((row['pickup_latitude'], row['pickup_longitude']), (row['dropoff_latitude'], row['dropoff_longitude'])))\n",
    "    dataframe['trip_distance'] = distance\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dd119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {
    "id": "93daa717"
   },
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd0d198",
   "metadata": {
    "id": "cbd0d198"
   },
   "outputs": [],
   "source": [
    "# get taxi taxi data from the website and generate a list of urls\n",
    "def find_taxi_parquet_urls() -> list:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    a = soup.find_all(lambda tag:'title' in tag.attrs and tag.attrs['title'] == \"Yellow Taxi Trip Records\")\n",
    "    hrefs = [link.get('href') for link in a]\n",
    "    hrefs_09 = list(filter(lambda href: '2009' in href, hrefs))\n",
    "    hrefs_10 = list(filter(lambda href: '2010' in href, hrefs))\n",
    "    hrefs_11 = list(filter(lambda href: '2011' in href, hrefs))\n",
    "    hrefs_12 = list(filter(lambda href: '2012' in href, hrefs))\n",
    "    hrefs_13 = list(filter(lambda href: '2013' in href, hrefs))\n",
    "    hrefs_14 = list(filter(lambda href: '2014' in href, hrefs))\n",
    "    hrefs_15 = list(filter(lambda href: '2015' in href, hrefs))\n",
    "    for i in range(6):\n",
    "        hrefs_15.pop()\n",
    "    hrefs_t = hrefs_09 + hrefs_10 + hrefs_11 + hrefs_12 + hrefs_13 + hrefs_14 + hrefs_15\n",
    "    return hrefs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f90831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_taxi_parquet_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e547623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58aa2793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-01 00:07:56</td>\n",
       "      <td>2012-01-01 00:12:09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>158</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-01 00:18:49</td>\n",
       "      <td>2012-01-01 00:30:01</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>231</td>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-01 00:31:38</td>\n",
       "      <td>2012-01-01 00:46:05</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>164</td>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-01 00:47:35</td>\n",
       "      <td>2012-01-01 00:55:57</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-01 00:57:08</td>\n",
       "      <td>2012-01-01 01:02:42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058343</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-31 23:43:30</td>\n",
       "      <td>2012-01-31 23:57:10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>162</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.90</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058344</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-31 23:04:26</td>\n",
       "      <td>2012-01-31 23:23:21</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058345</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-31 23:58:54</td>\n",
       "      <td>2012-02-01 00:05:33</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>237</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.48</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058346</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-31 23:10:40</td>\n",
       "      <td>2012-01-31 23:19:57</td>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>13</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058347</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-31 23:24:53</td>\n",
       "      <td>2012-01-31 23:41:48</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>158</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.60</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13058348 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          VendorID tpep_pickup_datetime tpep_dropoff_datetime  \\\n",
       "0                1  2012-01-01 00:07:56   2012-01-01 00:12:09   \n",
       "1                1  2012-01-01 00:18:49   2012-01-01 00:30:01   \n",
       "2                1  2012-01-01 00:31:38   2012-01-01 00:46:05   \n",
       "3                1  2012-01-01 00:47:35   2012-01-01 00:55:57   \n",
       "4                1  2012-01-01 00:57:08   2012-01-01 01:02:42   \n",
       "...            ...                  ...                   ...   \n",
       "13058343         1  2012-01-31 23:43:30   2012-01-31 23:57:10   \n",
       "13058344         1  2012-01-31 23:04:26   2012-01-31 23:23:21   \n",
       "13058345         1  2012-01-31 23:58:54   2012-02-01 00:05:33   \n",
       "13058346         1  2012-01-31 23:10:40   2012-01-31 23:19:57   \n",
       "13058347         1  2012-01-31 23:24:53   2012-01-31 23:41:48   \n",
       "\n",
       "          passenger_count  trip_distance  RatecodeID store_and_fwd_flag  \\\n",
       "0                       1            0.9           1                  N   \n",
       "1                       1            2.3           1                  N   \n",
       "2                       1            2.2           1                  N   \n",
       "3                       4            0.9           1                  N   \n",
       "4                       3            0.7           1                  N   \n",
       "...                   ...            ...         ...                ...   \n",
       "13058343                1            3.4           1                  N   \n",
       "13058344                1            4.3           1                  N   \n",
       "13058345                1            2.0           1                  N   \n",
       "13058346                1            3.4           1                  N   \n",
       "13058347                1            4.6           1                  N   \n",
       "\n",
       "          PULocationID  DOLocationID  payment_type  fare_amount  extra  \\\n",
       "0                  158           231             2          4.9    0.5   \n",
       "1                  231           164             2          8.5    0.5   \n",
       "2                  164           148             2          9.3    0.5   \n",
       "3                  148           107             2          5.3    0.5   \n",
       "4                  107           107             2          4.5    0.5   \n",
       "...                ...           ...           ...          ...    ...   \n",
       "13058343           162           231             1         10.9    0.5   \n",
       "13058344           148            50             2         14.5    0.5   \n",
       "13058345           237            75             1          6.9    0.5   \n",
       "13058346            13            68             1         10.1    0.5   \n",
       "13058347           158            49             1         13.7    0.5   \n",
       "\n",
       "          mta_tax  tip_amount  tolls_amount  improvement_surcharge  \\\n",
       "0             0.5        0.00           0.0                    0.0   \n",
       "1             0.5        0.00           0.0                    0.0   \n",
       "2             0.5        0.00           0.0                    0.0   \n",
       "3             0.5        0.00           0.0                    0.0   \n",
       "4             0.5        0.00           0.0                    0.0   \n",
       "...           ...         ...           ...                    ...   \n",
       "13058343      0.5        1.00           0.0                    0.0   \n",
       "13058344      0.5        0.00           0.0                    0.0   \n",
       "13058345      0.5        1.58           0.0                    0.0   \n",
       "13058346      0.5        3.00           0.0                    0.0   \n",
       "13058347      0.5        2.90           0.0                    0.0   \n",
       "\n",
       "          total_amount congestion_surcharge airport_fee  \n",
       "0                 5.90                 None        None  \n",
       "1                 9.50                 None        None  \n",
       "2                10.30                 None        None  \n",
       "3                 6.30                 None        None  \n",
       "4                 5.50                 None        None  \n",
       "...                ...                  ...         ...  \n",
       "13058343         12.90                 None        None  \n",
       "13058344         15.50                 None        None  \n",
       "13058345          9.48                 None        None  \n",
       "13058346         14.10                 None        None  \n",
       "13058347         17.60                 None        None  \n",
       "\n",
       "[13058348 rows x 19 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "GtxHDPSZ5Ejd",
   "metadata": {
    "id": "GtxHDPSZ5Ejd"
   },
   "outputs": [],
   "source": [
    "# Define a function that converts location to coordinates, and generate a dataframe\n",
    "def convert_id_to_coord(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    shapefile = gpd.read_file(r\"C:\\Users\\Silvia\\Downloads\\taxi_zones.shp\")\n",
    "    # Convert the geometry column in the shapefile into specific coordinates of latitude and longitude\n",
    "    shapefile = shapefile.to_crs(4326)\n",
    "    shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
    "    shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
    "    \n",
    "    df = df\n",
    "    df = df.loc[df[\"pulocationid\"] <= 263]\n",
    "    df = df.loc[df[\"pulocationid\"] != 0]\n",
    "    df = df.loc[df[\"dolocationid\"] <= 263]\n",
    "    df = df.loc[df[\"dolocationid\"] != 0]\n",
    "    # convert location IDs into longitude and latitude\n",
    "    PUlongitude = []\n",
    "    PUlatitude = []\n",
    "    DOlongitude = []\n",
    "    DOlatitude = []\n",
    "    # convert the pickup location IDs into longitude and latitude\n",
    "    for i in df['pulocationid']:\n",
    "        PUlatitude.append(shapefile['latitude'][i-1])\n",
    "        PUlongitude.append(shapefile['longitude'][i-1])\n",
    "    for i in df['dolocationid']:\n",
    "        DOlatitude.append(shapefile['latitude'][i-1])\n",
    "        DOlongitude.append(shapefile['longitude'][i-1])\n",
    "        \n",
    "    df['pickup_longitude'] = PUlongitude\n",
    "    df['pickup_latitude'] = PUlatitude\n",
    "    df['dropoff_longitude'] = DOlongitude\n",
    "    df['dropoff_latitude'] = DOlatitude\n",
    "    # convert the drop off location IDs into longitude and latitude\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lqS5XC4CUZhV",
   "metadata": {
    "id": "lqS5XC4CUZhV"
   },
   "outputs": [],
   "source": [
    "# Obtain taxi data and clean the data\n",
    "def get_and_clean_month_taxi_data(url: str) -> pd.core.frame.DataFrame:\n",
    "\n",
    "    df = pd.read_parquet(url)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df_taxi = pd.DataFrame()\n",
    "\n",
    "    # keep necessary columns into a new dataframe\n",
    "    if 'tpep_pickup_datetime' in df.columns:\n",
    "        df=df.rename(columns = {'tpep_pickup_datetime':'pickup_datetime',\n",
    "                                'tip_amount' : 'tip_amount'})\n",
    "        df=convert_id_to_coord(df)\n",
    "        \n",
    "    elif 'trip_pickup_datetime' in df.columns:\n",
    "        df=df.rename(columns = {'trip_pickup_datetime':'pickup_datetime', \n",
    "                                'start_lon': 'pickup_longitude',\n",
    "                                'start_lat': 'pickup_latitude',\n",
    "                                'end_lon': 'dropoff_longitude',\n",
    "                                'end_lat': 'dropoff_latitude',\n",
    "                                'tip_amt' : 'tip_amount'})\n",
    "        \n",
    "    df.drop(df.columns.difference(['pickup_datetime',\n",
    "                                    'trip_distance', \n",
    "                                    'pickup_latitude', \n",
    "                                    'pickup_longitude', \n",
    "                                    'dropoff_latitude', \n",
    "                                    'dropoff_longitude',\n",
    "                                   'tip_amount']), 1, inplace=True)\n",
    "    \n",
    "    df=df[df[\"pickup_longitude\"] <= -73.717047]  \n",
    "    df=df[df[\"pickup_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"pickup_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"pickup_latitude\"] <= 40.908524]\n",
    "    df=df[df[\"dropoff_longitude\"] <= -73.717047]\n",
    "    df=df[df[\"dropoff_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"dropoff_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"dropoff_latitude\"] <= 40.908524]\n",
    "\n",
    "    df = df.loc[df[\"pickup_datetime\"] != 0.0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {
    "id": "094b4d6d"
   },
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c58e3a2",
   "metadata": {
    "id": "7c58e3a2"
   },
   "outputs": [],
   "source": [
    "# load uber data and clean the data\n",
    "def load_and_clean_uber_data(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    df = pd.read_csv(csv_file, on_bad_lines='skip')\n",
    "    df.columns = df.columns.str.lower()\n",
    "    add_distance_column(df)\n",
    "    df.drop(df.columns.difference(['pickup_datetime',\n",
    "                                     'trip_distance', \n",
    "                                     'pickup_latitude', \n",
    "                                     'pickup_longitude', \n",
    "                                     'dropoff_latitude', \n",
    "                                     'dropoff_longitude']), 1, inplace=True)\n",
    "\n",
    "    # remove rows start and/or end outside of the following latitude/longitude coordinate box: \n",
    "    # (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    df=df[df[\"pickup_longitude\"] <= -73.717047]  \n",
    "    df=df[df[\"pickup_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"pickup_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"pickup_latitude\"] <= 40.908524]\n",
    "    df=df[df[\"dropoff_longitude\"] <= -73.717047]\n",
    "    df=df[df[\"dropoff_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"dropoff_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"dropoff_latitude\"] <= 40.908524]\n",
    "\n",
    "    # remove invalid rows thtat pickup time is 0\n",
    "    df = df.loc[df[\"pickup_datetime\"] != 0.0]\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zK96TZQrW2pS",
   "metadata": {
    "id": "zK96TZQrW2pS"
   },
   "source": [
    "Sampling the taxi data according to the number of uber trips each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ZoZgjuDpVzxk",
   "metadata": {
    "id": "ZoZgjuDpVzxk"
   },
   "outputs": [],
   "source": [
    "# Get the number of uber data per month\n",
    "def get_number_to_sample() -> int:\n",
    "    uber = load_and_clean_uber_data(r\"C:\\Users\\Silvia\\Downloads\\uber_rides_sample.csv\")\n",
    "    uber.index = pd.to_datetime(uber['pickup_datetime'])\n",
    "    number_each_month = uber.groupby(by=[uber.index.year, uber.index.month]).size()\n",
    "    number = []\n",
    "    for i in range(2009,2015):\n",
    "        for j in range(1,13):\n",
    "            number.append(number_each_month[i][j])\n",
    "    for i in range(1,7):\n",
    "        number.append(number_each_month[2015][i])\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lIrkJI_AXsfk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIrkJI_AXsfk",
    "outputId": "210a387f-4b45-48d9-ec43-b35e076d4578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\1263730049.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    }
   ],
   "source": [
    "number = get_number_to_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5cc323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2504,\n",
       " 2261,\n",
       " 2665,\n",
       " 2531,\n",
       " 2619,\n",
       " 2495,\n",
       " 2454,\n",
       " 2386,\n",
       " 2448,\n",
       " 2606,\n",
       " 2479,\n",
       " 2629,\n",
       " 2618,\n",
       " 1909,\n",
       " 2338,\n",
       " 2690,\n",
       " 2739,\n",
       " 2544,\n",
       " 2536,\n",
       " 2204,\n",
       " 2422,\n",
       " 2573,\n",
       " 2455,\n",
       " 2466,\n",
       " 2346,\n",
       " 2511,\n",
       " 2768,\n",
       " 2607,\n",
       " 2681,\n",
       " 2738,\n",
       " 2515,\n",
       " 2340,\n",
       " 2577,\n",
       " 2797,\n",
       " 2623,\n",
       " 2599,\n",
       " 2592,\n",
       " 2642,\n",
       " 2822,\n",
       " 2740,\n",
       " 2722,\n",
       " 2524,\n",
       " 2516,\n",
       " 2548,\n",
       " 2611,\n",
       " 2617,\n",
       " 2525,\n",
       " 2672,\n",
       " 2598,\n",
       " 2417,\n",
       " 2739,\n",
       " 2677,\n",
       " 2547,\n",
       " 2558,\n",
       " 2455,\n",
       " 2237,\n",
       " 2547,\n",
       " 2668,\n",
       " 2556,\n",
       " 2497,\n",
       " 2451,\n",
       " 2398,\n",
       " 2697,\n",
       " 2646,\n",
       " 2600,\n",
       " 2411,\n",
       " 2285,\n",
       " 2139,\n",
       " 2340,\n",
       " 2609,\n",
       " 2351,\n",
       " 2331,\n",
       " 2188,\n",
       " 2169,\n",
       " 2324,\n",
       " 2302,\n",
       " 2417,\n",
       " 2114]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "m0FIKYUEakdZ",
   "metadata": {
    "id": "m0FIKYUEakdZ"
   },
   "outputs": [],
   "source": [
    "def get_sample_taxi_data() -> pd.core.frame.DataFrame:\n",
    "    sample_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    # do the sampling of taxi data according to the number of uber trip each month\n",
    "    for i in range(0, 20): \n",
    "        n = number[i]\n",
    "        url = all_parquet_urls[i]\n",
    "        df = get_and_clean_month_taxi_data(url)\n",
    "        df_sample = df.sample(n)\n",
    "        sample_taxi_dataframes.append(df_sample)\n",
    "\n",
    "    taxi_data = pd.concat(sample_taxi_dataframes)\n",
    "    taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    taxi_data = taxi_data.reset_index(drop = True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95911a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    }
   ],
   "source": [
    "sample20 = get_sample_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e0ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample20.to_csv('sample20.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95024a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_taxi_data78() -> pd.core.frame.DataFrame:\n",
    "    sample_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    # do the sampling of taxi data according to the number of uber trip each month\n",
    "    for i in range(70, 78): \n",
    "        n = number[i]\n",
    "        url = all_parquet_urls[i]\n",
    "        df = get_and_clean_month_taxi_data(url)\n",
    "        df_sample = df.sample(n)\n",
    "        sample_taxi_dataframes.append(df_sample)\n",
    "\n",
    "    taxi_data = pd.concat(sample_taxi_dataframes)\n",
    "    taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    taxi_data = taxi_data.reset_index(drop = True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a566de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    }
   ],
   "source": [
    "sample78 = get_sample_taxi_data78()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "609bb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample78.to_csv('sample78.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0f93621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_taxi_data70() -> pd.core.frame.DataFrame:\n",
    "    sample_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    # do the sampling of taxi data according to the number of uber trip each month\n",
    "    for i in range(60, 70): \n",
    "        n = number[i]\n",
    "        url = all_parquet_urls[i]\n",
    "        df = get_and_clean_month_taxi_data(url)\n",
    "        df_sample = df.sample(n)\n",
    "        sample_taxi_dataframes.append(df_sample)\n",
    "\n",
    "    taxi_data = pd.concat(sample_taxi_dataframes)\n",
    "    taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    taxi_data = taxi_data.reset_index(drop = True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97d2d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\4068850014.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shapefile['longitude'] = shapefile['geometry'].centroid.x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_15864\\2223629645.py:22: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    }
   ],
   "source": [
    "sample70 = get_sample_taxi_data70()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d89295",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample70.to_csv('sample70.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46222ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-10 13:23:16</td>\n",
       "      <td>2.000</td>\n",
       "      <td>-73.982541</td>\n",
       "      <td>40.763843</td>\n",
       "      <td>-73.987940</td>\n",
       "      <td>40.741227</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-14 17:39:16</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-73.995395</td>\n",
       "      <td>40.749472</td>\n",
       "      <td>-74.004407</td>\n",
       "      <td>40.742506</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-26 06:21:00</td>\n",
       "      <td>19.260</td>\n",
       "      <td>-73.776892</td>\n",
       "      <td>40.645983</td>\n",
       "      <td>-73.982210</td>\n",
       "      <td>40.772557</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07 07:56:59</td>\n",
       "      <td>15.400</td>\n",
       "      <td>-73.863361</td>\n",
       "      <td>40.769973</td>\n",
       "      <td>-74.008209</td>\n",
       "      <td>40.703672</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-15 17:54:00</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-73.985552</td>\n",
       "      <td>40.747653</td>\n",
       "      <td>-73.966978</td>\n",
       "      <td>40.768760</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49650</th>\n",
       "      <td>2010-08-23 19:03:56</td>\n",
       "      <td>3.500</td>\n",
       "      <td>-73.991474</td>\n",
       "      <td>40.750012</td>\n",
       "      <td>-73.953978</td>\n",
       "      <td>40.772962</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49651</th>\n",
       "      <td>2010-08-01 17:27:36</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-73.990118</td>\n",
       "      <td>40.734483</td>\n",
       "      <td>-73.974515</td>\n",
       "      <td>40.753011</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49652</th>\n",
       "      <td>2010-08-23 04:34:02</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-73.983996</td>\n",
       "      <td>40.725399</td>\n",
       "      <td>-73.996840</td>\n",
       "      <td>40.712136</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49653</th>\n",
       "      <td>2010-08-08 19:19:00</td>\n",
       "      <td>4.610</td>\n",
       "      <td>-73.968080</td>\n",
       "      <td>40.802467</td>\n",
       "      <td>-73.989368</td>\n",
       "      <td>40.756822</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49654</th>\n",
       "      <td>2010-08-14 13:12:00</td>\n",
       "      <td>1.810</td>\n",
       "      <td>-73.976733</td>\n",
       "      <td>40.727043</td>\n",
       "      <td>-73.978187</td>\n",
       "      <td>40.744115</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49655 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickup_datetime  trip_distance  pickup_longitude  pickup_latitude  \\\n",
       "0     2009-01-10 13:23:16          2.000        -73.982541        40.763843   \n",
       "1     2009-01-14 17:39:16          1.300        -73.995395        40.749472   \n",
       "2     2009-01-26 06:21:00         19.260        -73.776892        40.645983   \n",
       "3     2009-01-07 07:56:59         15.400        -73.863361        40.769973   \n",
       "4     2009-01-15 17:54:00          0.171        -73.985552        40.747653   \n",
       "...                   ...            ...               ...              ...   \n",
       "49650 2010-08-23 19:03:56          3.500        -73.991474        40.750012   \n",
       "49651 2010-08-01 17:27:36          1.700        -73.990118        40.734483   \n",
       "49652 2010-08-23 04:34:02          1.400        -73.983996        40.725399   \n",
       "49653 2010-08-08 19:19:00          4.610        -73.968080        40.802467   \n",
       "49654 2010-08-14 13:12:00          1.810        -73.976733        40.727043   \n",
       "\n",
       "       dropoff_longitude  dropoff_latitude  tip_amount  \n",
       "0             -73.987940         40.741227        1.92  \n",
       "1             -74.004407         40.742506        3.00  \n",
       "2             -73.982210         40.772557        0.00  \n",
       "3             -74.008209         40.703672        0.00  \n",
       "4             -73.966978         40.768760        0.00  \n",
       "...                  ...               ...         ...  \n",
       "49650         -73.953978         40.772962        0.00  \n",
       "49651         -73.974515         40.753011        0.00  \n",
       "49652         -73.996840         40.712136        1.12  \n",
       "49653         -73.989368         40.756822        0.00  \n",
       "49654         -73.978187         40.744115        0.00  \n",
       "\n",
       "[49655 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60f79c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allsample = pd.concat([sample20, sample40, sample50, sample60, sample70, sample78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5e839d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-10 13:23:16</td>\n",
       "      <td>2.000</td>\n",
       "      <td>-73.982541</td>\n",
       "      <td>40.763843</td>\n",
       "      <td>-73.987940</td>\n",
       "      <td>40.741227</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-14 17:39:16</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-73.995395</td>\n",
       "      <td>40.749472</td>\n",
       "      <td>-74.004407</td>\n",
       "      <td>40.742506</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-26 06:21:00</td>\n",
       "      <td>19.260</td>\n",
       "      <td>-73.776892</td>\n",
       "      <td>40.645983</td>\n",
       "      <td>-73.982210</td>\n",
       "      <td>40.772557</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07 07:56:59</td>\n",
       "      <td>15.400</td>\n",
       "      <td>-73.863361</td>\n",
       "      <td>40.769973</td>\n",
       "      <td>-74.008209</td>\n",
       "      <td>40.703672</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-15 17:54:00</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-73.985552</td>\n",
       "      <td>40.747653</td>\n",
       "      <td>-73.966978</td>\n",
       "      <td>40.768760</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18191</th>\n",
       "      <td>2015-06-01 12:25:22</td>\n",
       "      <td>2.880</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-74.008984</td>\n",
       "      <td>40.735035</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18192</th>\n",
       "      <td>2015-06-07 23:02:31</td>\n",
       "      <td>12.100</td>\n",
       "      <td>-73.786533</td>\n",
       "      <td>40.646985</td>\n",
       "      <td>-73.917711</td>\n",
       "      <td>40.700522</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18193</th>\n",
       "      <td>2015-06-18 07:41:01</td>\n",
       "      <td>0.840</td>\n",
       "      <td>-73.984196</td>\n",
       "      <td>40.759818</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18194</th>\n",
       "      <td>2015-06-09 12:59:25</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18195</th>\n",
       "      <td>2015-06-13 04:10:05</td>\n",
       "      <td>4.800</td>\n",
       "      <td>-73.996919</td>\n",
       "      <td>40.720889</td>\n",
       "      <td>-73.939287</td>\n",
       "      <td>40.674469</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195472 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickup_datetime  trip_distance  pickup_longitude  pickup_latitude  \\\n",
       "0     2009-01-10 13:23:16          2.000        -73.982541        40.763843   \n",
       "1     2009-01-14 17:39:16          1.300        -73.995395        40.749472   \n",
       "2     2009-01-26 06:21:00         19.260        -73.776892        40.645983   \n",
       "3     2009-01-07 07:56:59         15.400        -73.863361        40.769973   \n",
       "4     2009-01-15 17:54:00          0.171        -73.985552        40.747653   \n",
       "...                   ...            ...               ...              ...   \n",
       "18191 2015-06-01 12:25:22          2.880        -73.977698        40.758028   \n",
       "18192 2015-06-07 23:02:31         12.100        -73.786533        40.646985   \n",
       "18193 2015-06-18 07:41:01          0.840        -73.984196        40.759818   \n",
       "18194 2015-06-09 12:59:25          0.500        -73.977698        40.758028   \n",
       "18195 2015-06-13 04:10:05          4.800        -73.996919        40.720889   \n",
       "\n",
       "       dropoff_longitude  dropoff_latitude  tip_amount  \n",
       "0             -73.987940         40.741227        1.92  \n",
       "1             -74.004407         40.742506        3.00  \n",
       "2             -73.982210         40.772557        0.00  \n",
       "3             -74.008209         40.703672        0.00  \n",
       "4             -73.966978         40.768760        0.00  \n",
       "...                  ...               ...         ...  \n",
       "18191         -74.008984         40.735035        0.00  \n",
       "18192         -73.917711         40.700522        7.46  \n",
       "18193         -73.977698         40.758028        0.00  \n",
       "18194         -73.970443         40.749914        1.95  \n",
       "18195         -73.939287         40.674469        2.00  \n",
       "\n",
       "[195472 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f292c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "allsample.to_csv('taxi_sample.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {
    "id": "45a15cbb"
   },
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {
    "id": "76e864ab"
   },
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    # read file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    #drop unnecessary colums\n",
    "    df.drop(df.columns.difference(['DATE',\n",
    "                                   'HourlyPrecipitation', \n",
    "                                   'HourlyWindSpeed']), 1, inplace=True)\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.0)\n",
    "    # drop na values\n",
    "    df.dropna(subset=['HourlyWindSpeed'], inplace=True)\n",
    "    # convert \"DATE\" to datetime type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    # convert \"HourlyPrecipitation\" to float type\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    # fill in missing values\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    # cast \"df\" to specified type\n",
    "    df = df.astype({'HourlyWindSpeed': 'float32', 'HourlyPrecipitation': 'float32'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {
    "id": "0687581f"
   },
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    # read file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Replace data of the string type\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.0)\n",
    "    # convert \"DATE\" to datetime type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    # convert \"HourlyPrecipitation\" to numeric type\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    # convert value of 'na' into 0.0\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    #drop unnecessary colums\n",
    "    df.drop(df.columns.difference(['DATE',\n",
    "                                   'HourlyPrecipitation', \n",
    "                                   'HourlyWindSpeed']), 1, inplace=True)\n",
    "    # calculate hourly average as a daily values\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df = df.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': np.mean, 'HourlyPrecipitation': np.mean})\n",
    "    df['HourlyWindSpeed'] = df['HourlyWindSpeed'].map(lambda x: round(x, 2))\n",
    "    # remame columns\n",
    "    df.rename(columns={'HourlyWindSpeed': 'DailyAverageWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    df = df.astype({'DailyAverageWindSpeed':'float32', 'DailyPrecipitation':'float32', 'DATE' : 'datetime64[ns]'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {
    "id": "3ef8945d"
   },
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> pd.core.frame.DataFrame:\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = [\n",
    "            \"/content/2009_weather.csv\",\n",
    "            \"/content/2010_weather.csv\",\n",
    "            \"/content/2011_weather.csv\",\n",
    "            \"/content/2012_weather.csv\",\n",
    "            \"/content/2013_weather.csv\",\n",
    "            \"/content/2014_weather.csv\",\n",
    "            \"/content/2015_weather.csv\"\n",
    "        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sunset_sunrise_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.astype({'Sunrise': 'int32', 'Sunset': 'int32', 'DATE':'datetime64[ns]' })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_sunrise_sunset_data() -> pd.core.frame.DataFrame:\n",
    "    sunrise_sunset_dataframes =[]\n",
    "    \n",
    "    weather_csv_files = [\n",
    "            \"/content/2009_weather.csv\",\n",
    "            \"/content/2010_weather.csv\",\n",
    "            \"/content/2011_weather.csv\",\n",
    "            \"/content/2012_weather.csv\",\n",
    "            \"/content/2013_weather.csv\",\n",
    "            \"/content/2014_weather.csv\",\n",
    "            \"/content/2015_weather.csv\"\n",
    "        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        sunrise_sunset_dataframe = clean_sunset_sunrise_daily(csv_file)\n",
    "        sunrise_sunset_dataframes.append(sunrise_sunset_dataframe)\n",
    "        \n",
    "    sunrise_sunset_data = pd.concat(sunrise_sunset_dataframes)\n",
    "    sunrise_sunset_data['DATE'] = pd.to_datetime(sunrise_sunset_data['DATE'])\n",
    "    sunrise_sunset_data = sunrise_sunset_data.astype({'Sunrise': 'int32', 'Sunset': 'int32'})\n",
    "    \n",
    "    return sunrise_sunset_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {
    "id": "f900f7aa"
   },
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {
    "id": "f7cd53a6"
   },
   "outputs": [],
   "source": [
    "taxi_data = get_sample_taxi_data()\n",
    "uber_data = load_and_clean_uber_data(\"/content/uber_rides_sample.csv\")\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()\n",
    "sunrise_sunset_data = load_and_clean_sunrise_sunset_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {
    "id": "dd101f11"
   },
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {
    "id": "f3529cf6"
   },
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Date, Integer, Float, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2286db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourlyWeather(Base):\n",
    "    __tablename__ = 'hourly_weathers'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Date = Column(Date)\n",
    "    HourlyPrecipitation = Column(Float)\n",
    "    HourlyWindSpeed = Column(Float)\n",
    "\n",
    "class DailyWeather(Base):\n",
    "    __tablename__ = 'daily_weathers'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Date = Column(Date)\n",
    "    DailyPrecipitation = Column(Float)\n",
    "    DailyAverageWindSpeed = Column(Float)\n",
    "\n",
    "class TaxiTrip(Base):\n",
    "    __tablename__ = 'taxi_trips'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    pickup_datetime = Column(Date)\n",
    "    trip_distance = Column(Float)\n",
    "    pickup_longitude = Column(Float)\n",
    "    pickup_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "\n",
    "class UberTrip(Base):\n",
    "    __tablename__ = 'uber_trips'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    pickup_datetime = Column(Date)\n",
    "    trip_distance = Column(Float)\n",
    "    pickup_longitude = Column(Float)\n",
    "    pickup_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oC2SH-VlxEAJ",
   "metadata": {
    "id": "oC2SH-VlxEAJ"
   },
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    weatherId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Date DATE,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    weatherId INTEGER PRIMARY KEY AUTOINCREMENT\n",
    "    Date DATE,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailyAverageWindSpeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    taxi_tripId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    distance FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    uber_tripId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    distance FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chAPoBRv12VX",
   "metadata": {
    "id": "chAPoBRv12VX"
   },
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(\"project.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {
    "id": "c122964f"
   },
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.to_sql(\"uber_trips\", con = connection,schema=UBER_TRIPS_SCHEMA)\n",
    "taxi_data.to_sql(\"taxi_trips\", con = connection,schema=TAXI_TRIPS_SCHEMA)\n",
    "daily_weather_data.to_sql(\"daily_weathers\", con = connection,schema=DAILY_WEATHER_SCHEMA)\n",
    "hourly_weather_data.to_sql(\"hourly_weathers\", con = connection,schema=HOURLY_WEATHER_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {
    "id": "8cb6e33e"
   },
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {
    "id": "b4753fcd"
   },
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {
    "id": "6a849e92"
   },
   "outputs": [],
   "source": [
    "def write_query_to_file(query: str, outfile: str):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {
    "id": "ee70a777"
   },
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef95e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT strftime ('%H',pickup_datetime) AS HOUR,\n",
    "COUNT(strftime ('%H',pickup_datetime)) AS Trip_counts\n",
    "FROM taxi_trips\n",
    "GROUP BY HOUR\n",
    "ORDER BY Trip_counts DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"Q1.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741813a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = '''\n",
    "SELECT strftime ('%w',pickup_datetime) AS Day,\n",
    "COUNT(strftime ('%w',pickup_datetime)) AS Trip_counts\n",
    "FROM uber_trips\n",
    "GROUP BY Day\n",
    "ORDER BY Trip_counts DESC\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aed09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"Q2.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ec601",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "SELECT distance FROM(\n",
    "SELECT trip_distance AS distance\n",
    "FROM (SELECT pickup_datetime, trip_distance, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude from taxi_trips\n",
    "WHERE pickup_datetime between '2013-07-01' AND '2013-07-31'\n",
    "union all\n",
    "SELECT pickup_datetime, trip_distance, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude from uber_trips)\n",
    "WHERE pickup_datetime between '2013-07-01' AND '2013-07-31'\n",
    ")\n",
    "ORDER BY distance ASC\n",
    "LIMIT 1\n",
    "OFFSET (SELECT count(*)\n",
    "FROM (SELECT trip_distance AS distance\n",
    "FROM (SELECT pickup_datetime, trip_distance, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude from taxi_trips\n",
    "WHERE pickup_datetime between '2013-07-01' AND '2013-07-31'\n",
    "union all\n",
    "SELECT pickup_datetime, trip_distance, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude from uber_trips)\n",
    "WHERE pickup_datetime between '2013-07-01' AND '2013-07-31'\n",
    ")) * 95 / 100 -1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f952538",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c57bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"Q3.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee97419",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "SELECT strftime ('%Y-%m-%d',pickup_datetime) AS DAY,\n",
    "AVG(trip_distance)\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime between '2009-01-01' AND '2009-12-31'\n",
    "GROUP BY DAY\n",
    "ORDER BY COUNT(strftime ('%Y-%m-%d',pickup_datetime)) DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e36578",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"Q4.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "SELECT DAY, COUNT(*) FROM\n",
    "(SELECT strftime('%Y-%m-%d', pickup_datetime) as DAY from taxi_trips\n",
    "UNION ALL\n",
    "SELECT strftime('%Y-%m-%d', pickup_datetime) as DAY from uber_trips)\n",
    "WHERE DAY IN (SELECT strftime ('%Y-%m-%d', DAY) AS DAY FROM daily_weathers\n",
    "WHERE DAY between '2014-01-01' AND '2014-12-31'\n",
    "ORDER BY (DailyAverageWindSpeed) DESC\n",
    "LIMIT 10)\n",
    "GROUP BY DAY\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_afile(QUERY_5, \"Q5.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "SELECT weathers.DATE, weathers.HOUR, weathers.HourlyPrecipitation, weathers.HourlyWindSpeed, trips.numbers\n",
    "FROM\n",
    "(\n",
    "SELECT strftime ('%Y-%m-%d',Date) AS DATE, strftime ('%H',Date) AS HOUR, HourlyPrecipitation, HourlyWindSpeed\n",
    "FROM hourly_weathers \n",
    "WHERE DATE BETWEEN '2012-10-22' AND '2012-11-07'\n",
    "GROUP BY strftime ('%Y-%m-%d',Date), strftime ('%H', Date), HourlyPrecipitation, HourlyWindSpeed\n",
    ") weathers\n",
    "LEFT JOIN\n",
    "(\n",
    "SELECT strftime ('%Y-%m-%d',pickup_datetime) AS DATE, strftime ('%H',pickup_datetime) AS HOUR, COUNT(strftime ('%H',pickup_datetime)) as numbers FROM taxi_trips\n",
    "WHERE DATE BETWEEN '2012-10-22' AND '2012-11-07'\n",
    "GROUP BY strftime ('%Y-%m-%d',pickup_datetime), strftime ('%H',pickup_datetime) \n",
    "union all\n",
    "SELECT strftime ('%Y-%m-%d',pickup_datetime) AS DATE, strftime ('%H',pickup_datetime) AS HOUR, COUNT(strftime ('%H',pickup_datetime)) as numbers FROM uber_trips\n",
    "WHERE DATE BETWEEN '2012-10-22' AND '2012-11-07'\n",
    "GROUP BY strftime ('%Y-%m-%d',pickup_datetime), strftime ('%H',pickup_datetime) \n",
    ") trips\n",
    "ON trips.DATE = weathers.DATE AND trips.HOUR = weathers.HOUR\n",
    "GROUP BY weathers.DATE, weathers.HOUR, weathers.HourlyPrecipitation, weathers.HourlyWindSpeed, trips.numbers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {
    "id": "a2ef04df"
   },
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"Q6.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {
    "id": "a13ced42"
   },
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {
    "id": "6d9eef42"
   },
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165d11d",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14864644",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_1 = \"\"\"\n",
    "SELECT strftime ('%H',pickup_datetime) AS HOUR,\n",
    "COUNT(strftime ('%H',pickup_datetime)) AS NUMBERS\n",
    "FROM taxi_trips\n",
    "GROUP BY HOUR\n",
    "\"\"\"\n",
    "\n",
    "def get_data_for_visual_1():\n",
    "    df = pd.read_sql(QUERY_4_1, con = engine)\n",
    "    return df\n",
    "\n",
    "dataframe = get_data_for_visual_1()\n",
    "\n",
    "def plot_visual_1(dataframe):\n",
    "    plt.bar(df['HOUR'],df['NUMBERS'])\n",
    "    plt.title('The hourly distribution of numbers of yellow taxi trips')\n",
    "    plt.xlabel('Hour of the day')\n",
    "    plt.ylabel('Number of yellow taxi trips')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae711418",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_2 = \"\"\"\n",
    "SELECT month,\n",
    "    total_trips,\n",
    "    AVG(avr_distance) as average_distance,\n",
    "    (count(distance)*sum(distance*distance) - (sum(distance)*sum(distance)))/((count(distance)-1)*(count(distance))) as variance\n",
    "    FROM \n",
    "    (SELECT \n",
    "    trip_distance AS distance,\n",
    "    strftime ('%m',pickup_datetime) AS month,\n",
    "    count(*) AS\ttotal_trips,\n",
    "    AVG(trip_distance) AS avr_distance\n",
    "    FROM \n",
    "    taxi_trips\n",
    "    GROUP BY \n",
    "    strftime ('%m',pickup_datetime)\n",
    "    UNION \n",
    "    SELECT \n",
    "    trip_distance AS distance,\n",
    "    strftime ('%m',pickup_datetime) AS month,\n",
    "    count(*) AS\ttotal_trips,\n",
    "    AVG(trip_distance) AS avr_distance\n",
    "    FROM \n",
    "    uber_trips\n",
    "    GROUP BY \n",
    "    strftime ('%m',pickup_datetime)\n",
    "    )trips\n",
    "    GROUP BY\n",
    "    trips.month\n",
    "\"\"\"\n",
    "\n",
    "def get_data_for_visual_2():\n",
    "    df = pd.read_sql(QUERY_4_2, con = engine)\n",
    "    return df\n",
    "\n",
    "df = get_data_for_visual_2()\n",
    "\n",
    "ci_lower = df['average_distance'] - 1.64 *np.sqrt(df['variance'])/np.sqrt(df[\"total_trips\"])\n",
    "ci_upper = df['average_distance'] + 1.64 *np.sqrt(df['variance'])/np.sqrt(df[\"total_trips\"])\n",
    "fig, ax = plt.subplots()\n",
    "x = df['month']\n",
    "ax.plot(x, df['average_distance'])\n",
    "ax.fill_between(x, ci_lower, ci_upper, color='b', alpha=.15)\n",
    "ax.set_ylim(ymin=2.5)\n",
    "ax.set_title('average distance traveled per month')\n",
    "fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba121e",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ac516",
   "metadata": {},
   "outputs": [],
   "source": [
    "JFK_BOX = [-73.825248, 40.620479, -73.746971, 40.666458]\n",
    "LGA_BOX = [-73.890716,40.767245,-73.854667,40.781415]\n",
    "EWR_BOX = [-74.192324,40.670659,-74.153185, 40.708601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e539c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_3():\n",
    "    query_JFK = \"\"\"\n",
    "    SELECT day, COUNT(day) AS Trip_counts\n",
    "    FROM\n",
    "    (SELECT strftime('%w', pickup_datetime) AS day, dropoff_longitude, dropoff_latitude from taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT strftime('%w', pickup_datetime) AS day, dropoff_longitude, dropoff_latitude from uber_trips)\n",
    "    WHERE dropoff_longitude BETWEEN -73.825248 AND -73.746971 AND dropoff_latitude BETWEEN 40.620479 AND 40.666458\n",
    "    GROUP BY day\n",
    "    ORDER BY Trip_counts\n",
    "    \"\"\"\n",
    "    query_LGA = \"\"\"\n",
    "    SELECT day, COUNT(day) AS Trip_counts\n",
    "    FROM\n",
    "    (SELECT strftime('%w', pickup_datetime) AS day, dropoff_longitude, dropoff_latitude from taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT strftime('%w', pickup_datetime) AS day, dropoff_longitude, dropoff_latitude from uber_trips)\n",
    "    WHERE dropoff_longitude BETWEEN -73.890716 AND -73.854667 AND dropoff_latitude BETWEEN 40.767245 AND 40.781415\n",
    "    GROUP BY day\n",
    "    ORDER BY Trip_counts\n",
    "    \"\"\"\n",
    "\n",
    "    query_EWR = \"\"\"\n",
    "    SELECT day, COUNT(day) AS Trip_counts\n",
    "    FROM\n",
    "    (SELECT strftime('%w', pickup_datetime) AS day, dropoff_longitude, dropoff_latitude from taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT strftime('%w', pickup_datetime) AS day, dropoff_longitude, dropoff_latitude from uber_trips)\n",
    "    WHERE dropoff_longitude BETWEEN -74.192324 AND -74.153185 AND dropoff_latitude BETWEEN 40.670659 AND 40.708601\n",
    "    GROUP BY day\n",
    "    ORDER BY Trip_counts\n",
    "    \"\"\"\n",
    "\n",
    "    df_JFK = pd.read_sql(query_JFK, con = engine)\n",
    "    df_LGA = pd.read_sql(query_LGA, con = engine)\n",
    "    df_EWR = pd.read_sql(query_EWR, con = engine)\n",
    "    \n",
    "    barWidth = 0.25\n",
    "    fig = plt.subplots(figsize =(12, 8))\n",
    "    br1 = np.arange(len(df_JFK['day']))\n",
    "    br2 = [x + barWidth for x in br1]\n",
    "    br3 = [x + barWidth for x in br2]\n",
    "\n",
    "    plt.bar(br1, df_JFK['Trip_counts'], color ='r', width = barWidth,\n",
    "            edgecolor ='grey', label ='JFK')\n",
    "    plt.bar(br2, df_LGA['Trip_counts'], color ='g', width = barWidth,\n",
    "            edgecolor ='grey', label ='LGA')\n",
    "    plt.bar(br3, df_EWR['Trip_counts'], color ='b', width = barWidth,\n",
    "            edgecolor ='grey', label ='EWR')\n",
    "\n",
    "    # Adding Xticks\n",
    "    plt.xlabel('Days in Week', fontweight ='bold', fontsize = 15)\n",
    "    plt.ylabel('Trip_counts', fontweight ='bold', fontsize = 15)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c5682",
   "metadata": {},
   "source": [
    "### Visualization 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd45c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map = KeplerGl(height = 500)\n",
    "get_map.add_data(uber_data, name = \"heatmap\")\n",
    "get_map.add_data(taxi_data, name = \"heatmap\")\n",
    "get_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bf418",
   "metadata": {},
   "source": [
    "### Visualization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_5():\n",
    "    query_visual_5 = \"\"\"\n",
    "    SELECT tip_amount, trip_distance FROM taxi_trips\n",
    "    \"\"\"\n",
    "\n",
    "    df_tip_distance = pd.read_sql(query_visual_5, con = engine)\n",
    "    plt.scatter(x = df_tip_distance['tip_amount'], y = df_tip_distance['trip_distance'])\n",
    "    plt.xlabel('tip_amount') \n",
    "    plt.ylabel('trip_distance') \n",
    "    plt.title('tip_amount vs. trip_distance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54514d33",
   "metadata": {},
   "source": [
    "### Visualization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6df4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_6():\n",
    "    query_taxi =\"\"\"SELECT strftime('%Y-%m-%d %H', pickup_datetime) AS DATE, tip_amount\n",
    "            FROM taxi_trips\n",
    "            \"\"\" \n",
    "    query_weather =\"\"\"SELECT strftime('%Y-%m-%d %H', DATE) AS DATE, HourlyPrecipitation\n",
    "            FROM hourly_weathers\n",
    "            \"\"\"\n",
    "\n",
    "    taxi_data = pd.read_sql_query(query_taxi, con = engine)\n",
    "    weather_data = pd.read_sql_query(query_weather, con = engine)\n",
    "    \n",
    "    taxi_data['DATE'] = pd.to_datetime(taxi_data['DATE'], format='%Y-%m-%d %H')\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'], format='%Y-%m-%d %H')\n",
    "\n",
    "    df = pd.merge(taxi_data, weather_data, on='DATE')\n",
    "    df = df[(df['HourlyPrecipitation'] > 0) & (df['tip_amount'] < 50)]\n",
    "    \n",
    "    # sample the data to make the scatter plot clearer\n",
    "    df.plot(x=\"HourlyPrecipitation\", y=\"tip_amount\", kind=\"scatter\", title=\"tip_amount vs. Precipitation\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
